# åœ°æ–¹ç«¶é¦¬ äºˆæ¸¬API
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional
import pandas as pd
import numpy as np
import pickle
import requests
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime
import os
import json
from pathlib import Path
from collections import defaultdict
import asyncio

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
BASE_DIR = Path(__file__).resolve().parent.parent

app = FastAPI(
    title="åœ°æ–¹ç«¶é¦¬äºˆæ¸¬API",
    description="AIãŒäºˆæ¸¬ã™ã‚‹åœ°æ–¹ç«¶é¦¬ã®3ç€ä»¥å†…äºˆæ¸¬",
    version="1.0.0"
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æœ¬ç•ªç’°å¢ƒã§ã¯é©åˆ‡ã«åˆ¶é™
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========== ç«¶é¦¬å ´è¨­å®š ==========
TRACKS = {
    "44": {"name": "å¤§äº•", "model": "models/model_ohi.pkl", "emoji": "ğŸŸï¸"},
    "45": {"name": "å·å´", "model": "models/model_kawasaki.pkl", "emoji": "ğŸŒŠ"},
    "43": {"name": "èˆ¹æ©‹", "model": "models/model_funabashi.pkl", "emoji": "âš“"},
    "42": {"name": "æµ¦å’Œ", "model": "models/model_urawa.pkl", "emoji": "ğŸŒ¸"},
    "30": {"name": "é–€åˆ¥", "model": "models/model_monbetsu.pkl", "emoji": "ğŸ´"},
    "35": {"name": "ç››å²¡", "model": "models/model_morioka.pkl", "emoji": "â›°ï¸"},
    "36": {"name": "æ°´æ²¢", "model": "models/model_mizusawa.pkl", "emoji": "ğŸ’§"},
    "46": {"name": "é‡‘æ²¢", "model": "models/model_kanazawa.pkl", "emoji": "âœ¨"},
    "47": {"name": "ç¬ æ¾", "model": "models/model_kasamatsu.pkl", "emoji": "ğŸ‹"},
    "48": {"name": "åå¤å±‹", "model": "models/model_nagoya.pkl", "emoji": "ğŸ¯"},
    "50": {"name": "åœ’ç”°", "model": "models/model_sonoda.pkl", "emoji": "ğŸŒ³"},
    "51": {"name": "å§«è·¯", "model": "models/model_himeji.pkl", "emoji": "ğŸ°"},
    "54": {"name": "é«˜çŸ¥", "model": "models/model_kochi.pkl", "emoji": "ğŸ‹"},
    "55": {"name": "ä½è³€", "model": "models/model_saga.pkl", "emoji": "ğŸ‹"},
}

# ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
model_cache = {}

# ========== äºˆæ¸¬ãƒ­ã‚°ä¿å­˜ ==========
def save_prediction_log(race_id: str, track_code: str, predictions: list, metadata: dict = None):
    """äºˆæ¸¬çµæœã‚’JSONã«ä¿å­˜ï¼ˆå¾Œã§çµæœã¨ç…§åˆã™ã‚‹ãŸã‚ï¼‰"""
    try:
        # æ—¥ä»˜ã‚’æŠ½å‡ºï¼ˆrace_idã‹ã‚‰ï¼‰
        date_str = race_id[:4] + "-" + race_id[6:8] + "-" + race_id[8:10]

        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        log_dir = BASE_DIR / "prediction_logs" / date_str
        log_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        log_data = {
            "race_id": race_id,
            "track_code": track_code,
            "track_name": TRACKS.get(track_code, {}).get("name", "ä¸æ˜"),
            "predicted_at": datetime.now().isoformat(),
            "predictions": predictions,
            "metadata": metadata or {}
        }

        # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        log_file = log_dir / f"{race_id}.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)

        print(f"Prediction log saved: {log_file}")
    except Exception as e:
        print(f"Failed to save prediction log: {e}")

# æ—§ãƒ¢ãƒ‡ãƒ«åã¨ã®äº’æ›æ€§
MODEL_ALIASES = {
    "models/model_ohi.pkl": ["models/model_ohi.pkl", "model_v2.pkl"],
}


# ========== ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ ==========
class NARScraper:
    BASE_URL = "https://nar.netkeiba.com"
    DB_URL = "https://db.netkeiba.com"

    def __init__(self, track_code, delay=0.5):
        self.track_code = track_code
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})
        self.horse_cache = {}
        self.jockey_cache = {}

    def _fetch(self, url, encoding='EUC-JP'):
        time.sleep(self.delay)
        r = self.session.get(url)
        r.encoding = encoding
        return BeautifulSoup(r.text, 'lxml')

    def get_race_list_by_date(self, date: str) -> list:
        url = f"{self.BASE_URL}/top/race_list_sub.html?kaisai_date={date}"
        try:
            soup = self._fetch(url, encoding='UTF-8')
            ids = []
            for a in soup.find_all('a', href=True):
                m = re.search(r'race_id=(\d+)', a['href'])
                if m:
                    race_id = m.group(1)
                    if len(race_id) >= 6 and race_id[4:6] == self.track_code:
                        ids.append(race_id)
            return list(set(ids))
        except:
            return []

    def get_race_data(self, race_id: str):
        url = f"{self.BASE_URL}/race/shutuba.html?race_id={race_id}"
        try:
            soup = self._fetch(url)
            info = {'race_id': race_id}

            # ãƒ¬ãƒ¼ã‚¹å
            nm = soup.find('h1', class_='RaceName')
            if nm:
                info['race_name'] = nm.get_text(strip=True)

            # ç™ºèµ°æ™‚åˆ»
            rd = soup.find('div', class_='RaceData01')
            if rd:
                rd_text = rd.get_text()
                tm = re.search(r'(\d{1,2}):(\d{2})', rd_text)
                if tm:
                    info['start_time'] = f"{tm.group(1)}:{tm.group(2)}"
                dm = re.search(r'(\d{3,4})m', rd_text)
                if dm:
                    info['distance'] = int(dm.group(1))

                # é¦¬å ´çŠ¶æ…‹ã‚’æŠ½å‡ºï¼ˆè‰¯/ç¨é‡/é‡/ä¸è‰¯ï¼‰
                track_cond_match = re.search(r'[ãƒ€èŠ].*?[:ï¼š]\s*(è‰¯|ç¨é‡|é‡|ä¸è‰¯)', rd_text)
                if track_cond_match:
                    info['track_condition'] = track_cond_match.group(1)
                else:
                    info['track_condition'] = 'è‰¯'

                # å¤©æ°—ã‚’æŠ½å‡ºï¼ˆæ™´/æ›‡/é›¨/å°é›¨/é›ªï¼‰
                weather_match = re.search(r'å¤©æ°—[:ï¼š]\s*(æ™´|æ›‡|é›¨|å°é›¨|é›ª)', rd_text)
                if weather_match:
                    info['weather'] = weather_match.group(1)
                else:
                    info['weather'] = 'æ™´'

            # ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—
            table = soup.find('table', class_='ShutubaTable')
            if not table:
                table = soup.find('table', class_='RaceTable01')
            if not table:
                for t in soup.find_all('table'):
                    if t.find('a', href=re.compile(r'/horse/')):
                        table = t
                        break
            if not table:
                return None

            rows = []
            for tr in table.find_all('tr'):
                tds = tr.find_all('td')
                if len(tds) < 4:
                    continue

                data = info.copy()

                bracket_text = tds[0].get_text(strip=True)
                if bracket_text.isdigit():
                    data['bracket'] = int(bracket_text)
                umaban_text = tds[1].get_text(strip=True)
                if umaban_text.isdigit():
                    data['horse_number'] = int(umaban_text)

                horse_link = tr.find('a', href=re.compile(r'/horse/\d+'))
                if horse_link:
                    data['horse_name'] = horse_link.get_text(strip=True)
                    m = re.search(r'/horse/(\d+)', horse_link['href'])
                    if m:
                        data['horse_id'] = m.group(1)

                jockey_link = tr.find('a', href=re.compile(r'/jockey/'))
                if jockey_link:
                    data['jockey_name'] = jockey_link.get_text(strip=True)
                    m = re.search(r'/jockey/(?:result/recent/)?([a-zA-Z0-9]+)', jockey_link['href'])
                    if m:
                        data['jockey_id'] = m.group(1)

                # èª¿æ•™å¸«ã‚’æŠ½å‡º
                trainer_link = tr.find('a', href=re.compile(r'/trainer/'))
                if trainer_link:
                    data['trainer_name'] = trainer_link.get_text(strip=True)
                    m = re.search(r'/trainer/(?:result/recent/)?([a-zA-Z0-9]+)', trainer_link['href'])
                    if m:
                        data['trainer_id'] = m.group(1)

                # é¦¬ä½“é‡ã‚’æŠ½å‡ºï¼ˆä¾‹: 450(+4), 448(-2), 452ï¼‰
                for td in tds:
                    weight_text = td.get_text(strip=True)
                    weight_match = re.match(r'^(\d{3,4})(?:\(([+-]?\d+)\))?$', weight_text)
                    if weight_match and 300 <= int(weight_match.group(1)) <= 600:
                        data['horse_weight'] = int(weight_match.group(1))
                        if weight_match.group(2):
                            data['weight_change'] = int(weight_match.group(2))
                        else:
                            data['weight_change'] = 0
                        break

                for td in tds:
                    text = td.get_text(strip=True)
                    if re.match(r'^[ç‰¡ç‰ã‚»]\d$', text):
                        data['sex'] = text[0]
                        data['age'] = int(text[1])
                    if re.match(r'^\d{2}(\.\d)?$', text):
                        w = float(text)
                        if 45 <= w <= 65 and 'weight_carried' not in data:
                            data['weight_carried'] = w

                if data.get('horse_name'):
                    rows.append(data)

            if not rows:
                return None

            df = pd.DataFrame(rows)
            df['field_size'] = len(df)
            return df
        except Exception as e:
            print(f'Error: {e}')

    def get_all_odds(self, race_id: str) -> dict:
        """å˜å‹ãƒ»è¤‡å‹ã‚ªãƒƒã‚ºã‚’ä¸€æ‹¬å–å¾—ï¼ˆAPIå‘¼ã³å‡ºã—æœ€å°åŒ–ï¼‰"""
        result = {'win': {}, 'place': {}}

        # 1. å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‹ã‚‰å˜å‹ã‚ªãƒƒã‚ºã‚’å–å¾—ï¼ˆ1ãƒªã‚¯ã‚¨ã‚¹ãƒˆç›®ï¼‰
        shutuba_url = f"{self.BASE_URL}/race/shutuba.html?race_id={race_id}"
        try:
            soup = self._fetch(shutuba_url)
            table = soup.find('table', class_='ShutubaTable')
            if not table:
                table = soup.find('table', class_='RaceTable01')

            if table:
                for tr in table.find_all('tr'):
                    tds = tr.find_all('td')
                    if len(tds) >= 2:
                        umaban = None
                        odds_val = None

                        for i, td in enumerate(tds[:3]):
                            td_class = ' '.join(td.get('class', []))
                            text = td.get_text(strip=True)
                            if 'Umaban' in td_class or (i == 1 and text.isdigit()):
                                if text.isdigit() and 1 <= int(text) <= 18:
                                    umaban = int(text)
                                    break

                        for td in tds:
                            td_class = ' '.join(td.get('class', []))
                            if 'Popular' in td_class or 'Odds' in td_class or 'odds' in td_class.lower():
                                text = td.get_text(strip=True)
                                odds_match = re.search(r'(\d+\.?\d*)', text)
                                if odds_match:
                                    val = float(odds_match.group(1))
                                    if 1.0 <= val <= 999.9:
                                        odds_val = val
                                        break

                        if umaban and odds_val:
                            result['win'][umaban] = odds_val
        except Exception as e:
            print(f'Win odds error: {e}')

        # 2. è¤‡å‹ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ï¼ˆ2ãƒªã‚¯ã‚¨ã‚¹ãƒˆç›®ï¼‰
        place_url = f"{self.BASE_URL}/odds/odds_get_form.html?type=b2&race_id={race_id}"
        try:
            soup = self._fetch(place_url)
            tables = soup.find_all('table')
            if len(tables) >= 2:
                table = tables[1]
                for tr in table.find_all('tr'):
                    tds = tr.find_all('td')
                    # tdæ§‹é€ : [æ ç•ª, é¦¬ç•ª, ç©º, é¦¬å, ã‚ªãƒƒã‚º]
                    if len(tds) >= 5:
                        umaban_text = tds[1].get_text(strip=True)  # td[1]ãŒé¦¬ç•ª
                        if umaban_text.isdigit():
                            umaban = int(umaban_text)
                            # ã‚ªãƒƒã‚ºã¯æœ€å¾Œã®td
                            odds_text = tds[-1].get_text(strip=True)
                            # ã€Œ1.4 - 2.6ã€å½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
                            odds_match = re.search(r'(\d+\.?\d*)\s*-\s*(\d+\.?\d*)', odds_text)
                            if odds_match:
                                min_odds = float(odds_match.group(1))
                                max_odds = float(odds_match.group(2))
                                result['place'][umaban] = {
                                    'min': min_odds,
                                    'max': max_odds,
                                    'avg': round((min_odds + max_odds) / 2, 2)
                                }
                            else:
                                # å˜ä¸€ã®æ•°å€¤ã®å ´åˆ
                                single_match = re.search(r'(\d+\.?\d*)', odds_text)
                                if single_match:
                                    odds_val = float(single_match.group(1))
                                    result['place'][umaban] = {
                                        'min': odds_val,
                                        'max': odds_val,
                                        'avg': odds_val
                                    }
        except Exception as e:
            print(f'Place odds error: {e}')

        return result

    def get_odds(self, race_id: str, horse_names: list = None) -> dict:
        """å˜å‹ã‚ªãƒƒã‚ºã‚’å–å¾—ï¼ˆå‡ºé¦¬è¡¨ã®äºˆæƒ³ã‚ªãƒƒã‚ºåˆ—ã‹ã‚‰ï¼‰"""
        odds_dict = {}

        # 1. å‡ºé¦¬è¡¨ãƒšãƒ¼ã‚¸ã‹ã‚‰äºˆæƒ³ã‚ªãƒƒã‚ºã‚’å–å¾—ï¼ˆæœ€ã‚‚ç¢ºå®Ÿï¼‰
        shutuba_url = f"{self.BASE_URL}/race/shutuba.html?race_id={race_id}"
        try:
            soup = self._fetch(shutuba_url)
            table = soup.find('table', class_='ShutubaTable')
            if not table:
                table = soup.find('table', class_='RaceTable01')

            if table:
                for tr in table.find_all('tr'):
                    tds = tr.find_all('td')
                    if len(tds) >= 2:
                        # é¦¬ç•ªã¯é€šå¸¸2ç•ªç›®ã®tdï¼ˆ1ç•ªç›®ã¯æ ç•ªï¼‰
                        umaban = None
                        odds_val = None

                        # é¦¬ç•ªã‚’å–å¾—ï¼ˆUmabanã‚¯ãƒ©ã‚¹ã¾ãŸã¯2ç•ªç›®ã®tdï¼‰
                        for i, td in enumerate(tds[:3]):
                            td_class = ' '.join(td.get('class', []))
                            text = td.get_text(strip=True)
                            if 'Umaban' in td_class or (i == 1 and text.isdigit()):
                                if text.isdigit() and 1 <= int(text) <= 18:
                                    umaban = int(text)
                                    break

                        # äºˆæƒ³ã‚ªãƒƒã‚ºã‚’å–å¾—ï¼ˆPopularåˆ—ã€é€šå¸¸ã¯å¾Œã‚ã®æ–¹ã®tdï¼‰
                        for td in tds:
                            td_class = ' '.join(td.get('class', []))
                            # Popularã‚¯ãƒ©ã‚¹ã¾ãŸã¯oddsé–¢é€£ã®ã‚¯ãƒ©ã‚¹ã‚’æŒã¤ã‚»ãƒ«
                            if 'Popular' in td_class or 'Odds' in td_class or 'odds' in td_class.lower():
                                text = td.get_text(strip=True)
                                odds_match = re.search(r'(\d+\.?\d*)', text)
                                if odds_match:
                                    val = float(odds_match.group(1))
                                    if 1.0 <= val <= 999.9:
                                        odds_val = val
                                        break

                        if umaban and odds_val:
                            odds_dict[umaban] = odds_val

            if odds_dict:
                print(f"DEBUG shutuba odds: {odds_dict}")
                return odds_dict
        except Exception as e:
            print(f'Shutuba odds error: {e}')

        # ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã§ããªã‹ã£ãŸå ´åˆã€ã‚¹ãƒãƒ›ç‰ˆçµæœãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨é¦¬ã®ã‚ªãƒƒã‚ºã‚’å–å¾—
        # ã‚¹ãƒãƒ›ç‰ˆã¯çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã«å…¨é¦¬ã®å˜å‹ã‚ªãƒƒã‚ºãŒå«ã¾ã‚Œã¦ã„ã‚‹
        sp_result_url = f"https://nar.sp.netkeiba.com/race/race_result.html?race_id={race_id}"
        try:
            soup = self._fetch(sp_result_url, encoding='UTF-8')

            # ã‚¹ãƒãƒ›ç‰ˆã®çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã‚ªãƒƒã‚ºã‚’å–å¾—
            # ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã®å„è¡Œã‹ã‚‰é¦¬ç•ªã¨ã‚ªãƒƒã‚ºã‚’æŠ½å‡º
            for tr in soup.find_all('tr'):
                tds = tr.find_all('td')
                if len(tds) >= 8:
                    try:
                        # é¦¬ç•ªã‚’æ¢ã™ï¼ˆé€šå¸¸ã¯æœ€åˆã®æ–¹ã®tdï¼‰
                        umaban = None
                        odds_val = None

                        for i, td in enumerate(tds):
                            text = td.get_text(strip=True)
                            # é¦¬ç•ªï¼ˆ1-18ã®æ•°å­—ã€é€šå¸¸2æ¡ä»¥ä¸‹ï¼‰
                            if text.isdigit() and 1 <= int(text) <= 18 and umaban is None:
                                # ç€é †ã§ã¯ãªãé¦¬ç•ªã‹ã‚’ç¢ºèªï¼ˆç€é †ã¯1ã‹ã‚‰å§‹ã¾ã‚‹å°ã•ã„æ•°å­—ï¼‰
                                # classå±æ€§ã‚„dataå±æ€§ã§åˆ¤åˆ¥ã§ãã‚‹å ´åˆã‚‚ã‚ã‚‹
                                td_class = td.get('class', [])
                                if 'Umaban' in str(td_class) or i >= 1:
                                    umaban = int(text)

                        # ã‚ªãƒƒã‚ºã‚’æ¢ã™ï¼ˆå°æ•°ç‚¹ã‚’å«ã‚€æ•°å­—ï¼‰
                        for td in tds:
                            text = td.get_text(strip=True)
                            # ã‚ªãƒƒã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³: "1.5" or "29.8" ãªã©
                            odds_match = re.match(r'^(\d+\.\d+)$', text)
                            if odds_match:
                                val = float(odds_match.group(1))
                                if 1.0 <= val <= 999.9:
                                    odds_val = val
                                    break

                        if umaban and odds_val:
                            odds_dict[umaban] = odds_val

                    except (ValueError, IndexError):
                        continue

            if odds_dict:
                return odds_dict

        except Exception as e:
            print(f'SP result page error: {e}')

        # æœ€å¾Œã®æ‰‹æ®µ: PCç‰ˆçµæœãƒšãƒ¼ã‚¸ã®æ‰•æˆ»é‡‘ã‹ã‚‰å‹ã¡é¦¬ã®ã¿å–å¾—
        result_url = f"{self.BASE_URL}/race/result.html?race_id={race_id}"
        try:
            soup = self._fetch(result_url)
            payout_table = soup.find('table', class_='Payout_Detail_Table')
            if payout_table:
                for tr in payout_table.find_all('tr'):
                    th = tr.find('th')
                    if th and 'å˜å‹' in th.get_text():
                        tds = tr.find_all('td')
                        if len(tds) >= 2:
                            umaban_text = tds[0].get_text(strip=True)
                            payout_text = tds[1].get_text(strip=True)
                            if umaban_text.isdigit():
                                umaban = int(umaban_text)
                                payout_match = re.search(r'([\d,]+)', payout_text)
                                if payout_match:
                                    payout = int(payout_match.group(1).replace(',', ''))
                                    odds_dict[umaban] = payout / 100
            return odds_dict
        except Exception as e:
            print(f'Result page error: {e}')
            return {}

    def get_horse_history(self, horse_id: str):
        if horse_id in self.horse_cache:
            return self.horse_cache[horse_id]

        url = f"{self.DB_URL}/horse/ajax_horse_results.html?id={horse_id}"
        try:
            time.sleep(self.delay)
            r = self.session.get(url)
            r.encoding = 'EUC-JP'
            soup = BeautifulSoup(r.text, 'lxml')

            results = []
            for tr in soup.find_all('tr'):
                tds = tr.find_all('td')
                if len(tds) < 6:
                    continue
                for td in tds[3:7]:
                    t = td.get_text(strip=True)
                    if t.isdigit() and 1 <= int(t) <= 20:
                        results.append(int(t))
                        break
                if len(results) >= 20:
                    break

            stats = self._calc_stats(results)
            self.horse_cache[horse_id] = stats
            return stats
        except:
            return self._empty_stats()

    def get_jockey_stats(self, jockey_id: str):
        if jockey_id in self.jockey_cache:
            return self.jockey_cache[jockey_id]

        url = f"{self.DB_URL}/jockey/{jockey_id}/"
        try:
            soup = self._fetch(url)
            text = soup.get_text()
            stats = {'jockey_win_rate': 0, 'jockey_place_rate': 0, 'jockey_show_rate': 0}

            m = re.search(r'å‹ç‡[ï¼š:\s]*(\d+\.?\d*)', text)
            if m:
                stats['jockey_win_rate'] = float(m.group(1)) / 100
            m = re.search(r'é€£å¯¾ç‡[ï¼š:\s]*(\d+\.?\d*)', text)
            if m:
                stats['jockey_place_rate'] = float(m.group(1)) / 100
            m = re.search(r'è¤‡å‹ç‡[ï¼š:\s]*(\d+\.?\d*)', text)
            if m:
                stats['jockey_show_rate'] = float(m.group(1)) / 100

            self.jockey_cache[jockey_id] = stats
            return stats
        except:
            return {'jockey_win_rate': 0, 'jockey_place_rate': 0, 'jockey_show_rate': 0}

    def _calc_stats(self, ranks):
        if not ranks:
            return self._empty_stats()
        total = len(ranks)
        wins = sum(1 for r in ranks if r == 1)
        place = sum(1 for r in ranks if r <= 2)
        show = sum(1 for r in ranks if r <= 3)
        recent = ranks[:5]
        r_total = len(recent)
        return {
            'horse_runs': total,
            'horse_win_rate': wins / total,
            'horse_place_rate': place / total,
            'horse_show_rate': show / total,
            'horse_avg_rank': np.mean(ranks),
            'horse_recent_win_rate': sum(1 for r in recent if r == 1) / r_total if r_total else 0,
            'horse_recent_show_rate': sum(1 for r in recent if r <= 3) / r_total if r_total else 0,
            'horse_recent_avg_rank': np.mean(recent) if recent else 10,
            'last_rank': ranks[0] if ranks else 10
        }

    def _empty_stats(self):
        return {
            'horse_runs': 0, 'horse_win_rate': 0, 'horse_place_rate': 0,
            'horse_show_rate': 0, 'horse_avg_rank': 10,
            'horse_recent_win_rate': 0, 'horse_recent_show_rate': 0,
            'horse_recent_avg_rank': 10, 'last_rank': 10
        }

    def enrich_data(self, df):
        df = df.copy()
        if 'horse_id' in df.columns:
            horse_data = []
            for hid in df['horse_id'].dropna().unique():
                stats = self.get_horse_history(str(hid))
                stats['horse_id'] = hid
                horse_data.append(stats)
            if horse_data:
                hdf = pd.DataFrame(horse_data)
                df['horse_id'] = df['horse_id'].astype(str)
                hdf['horse_id'] = hdf['horse_id'].astype(str)
                df = df.merge(hdf, on='horse_id', how='left')

        if 'jockey_id' in df.columns:
            jockey_data = []
            for jid in df['jockey_id'].dropna().unique():
                stats = self.get_jockey_stats(str(jid))
                stats['jockey_id'] = jid
                jockey_data.append(stats)
            if jockey_data:
                jdf = pd.DataFrame(jockey_data)
                df['jockey_id'] = df['jockey_id'].astype(str)
                jdf['jockey_id'] = jdf['jockey_id'].astype(str)
                df = df.merge(jdf, on='jockey_id', how='left')
        return df


# ========== å‰å‡¦ç† ==========
class Processor:
    def __init__(self):
        self.features = [
            'horse_runs', 'horse_win_rate', 'horse_place_rate', 'horse_show_rate',
            'horse_avg_rank', 'horse_recent_win_rate', 'horse_recent_show_rate',
            'horse_recent_avg_rank', 'last_rank',
            'jockey_win_rate', 'jockey_place_rate', 'jockey_show_rate',
            'horse_number', 'bracket', 'age', 'weight_carried', 'distance',
            'sex_encoded', 'track_encoded', 'field_size', 'weight_diff',
            # æ–°ç‰¹å¾´é‡
            'track_condition_encoded', 'weather_encoded',
            'trainer_encoded', 'horse_weight', 'horse_weight_change'
        ]

    def process(self, df):
        df = df.copy()
        num_cols = ['bracket', 'horse_number', 'age', 'weight_carried', 'distance',
                    'field_size', 'horse_runs', 'horse_win_rate', 'horse_place_rate',
                    'horse_show_rate', 'horse_avg_rank', 'horse_recent_win_rate',
                    'horse_recent_show_rate', 'horse_recent_avg_rank', 'last_rank',
                    'jockey_win_rate', 'jockey_place_rate', 'jockey_show_rate',
                    'horse_weight', 'weight_change']
        for c in num_cols:
            if c in df.columns:
                df[c] = pd.to_numeric(df[c], errors='coerce')

        if 'sex' in df.columns:
            df['sex_encoded'] = df['sex'].map({'ç‰¡': 0, 'ç‰': 1, 'ã‚»': 2}).fillna(0)
        else:
            df['sex_encoded'] = 0

        df['track_encoded'] = 0

        if 'weight_carried' in df.columns and 'race_id' in df.columns:
            df['weight_diff'] = df.groupby('race_id')['weight_carried'].transform(lambda x: x - x.mean())
        else:
            df['weight_diff'] = 0

        if 'field_size' not in df.columns:
            if 'race_id' in df.columns:
                df['field_size'] = df.groupby('race_id')['race_id'].transform('count')
            else:
                df['field_size'] = 12

        # é¦¬å ´çŠ¶æ…‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆè‰¯=0, ç¨é‡=1, é‡=2, ä¸è‰¯=3ï¼‰
        if 'track_condition' in df.columns:
            df['track_condition_encoded'] = df['track_condition'].map(
                {'è‰¯': 0, 'ç¨é‡': 1, 'é‡': 2, 'ä¸è‰¯': 3}
            ).fillna(0)
        else:
            df['track_condition_encoded'] = 0

        # å¤©æ°—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆæ™´=0, æ›‡=1, å°é›¨=2, é›¨=3, é›ª=4ï¼‰
        if 'weather' in df.columns:
            df['weather_encoded'] = df['weather'].map(
                {'æ™´': 0, 'æ›‡': 1, 'å°é›¨': 2, 'é›¨': 3, 'é›ª': 4}
            ).fillna(0)
        else:
            df['weather_encoded'] = 0

        # èª¿æ•™å¸«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ï¼‰
        if 'trainer_id' in df.columns:
            df['trainer_encoded'] = df['trainer_id'].apply(
                lambda x: hash(str(x)) % 10000 if pd.notna(x) else 0
            )
        else:
            df['trainer_encoded'] = 0

        # é¦¬ä½“é‡ï¼ˆæ¬ æã¯450kgã§è£œå®Œï¼‰
        if 'horse_weight' in df.columns:
            df['horse_weight'] = df['horse_weight'].fillna(450)
        else:
            df['horse_weight'] = 450

        # é¦¬ä½“é‡å¢—æ¸›
        if 'weight_change' in df.columns:
            df['horse_weight_change'] = df['weight_change'].fillna(0)
        else:
            df['horse_weight_change'] = 0

        for f in self.features:
            if f not in df.columns:
                df[f] = 0
        return df

    def get_features(self):
        return self.features


# ========== ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ ==========
def load_model(track_code: str):
    if track_code in model_cache:
        return model_cache[track_code]

    if track_code not in TRACKS:
        return None, None

    model_name = TRACKS[track_code]['model']

    # ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæ—§ãƒ¢ãƒ‡ãƒ«åã¨ã®äº’æ›æ€§ï¼‰
    paths_to_try = [model_name]
    if model_name in MODEL_ALIASES:
        paths_to_try = MODEL_ALIASES[model_name]

    for model_name in paths_to_try:
        model_path = BASE_DIR / model_name
        if model_path.exists():
            with open(model_path, 'rb') as f:
                d = pickle.load(f)
            model_cache[track_code] = (d['model'], d['features'])
            return d['model'], d['features']
    return None, None


# ========== APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ==========

@app.get("/")
def root():
    return {"message": "åœ°æ–¹ç«¶é¦¬äºˆæ¸¬API", "version": "1.0.0"}


@app.get("/api/tracks")
def get_tracks():
    """åˆ©ç”¨å¯èƒ½ãªç«¶é¦¬å ´ä¸€è¦§ã‚’å–å¾—"""
    tracks = []
    for code, info in TRACKS.items():
        model_name = info['model']
        # ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚‚å«ã‚ã¦ãƒã‚§ãƒƒã‚¯
        paths_to_check = [model_name]
        if model_name in MODEL_ALIASES:
            paths_to_check = MODEL_ALIASES[model_name]
        model_exists = any((BASE_DIR / p).exists() for p in paths_to_check)
        tracks.append({
            "code": code,
            "name": info['name'],
            "emoji": info['emoji'],
            "model_available": model_exists
        })
    return {"tracks": tracks}


class PredictRequest(BaseModel):
    track_code: str
    date: str  # YYYY-MM-DDå½¢å¼


class PredictionResult(BaseModel):
    rank: int
    number: int
    name: str
    jockey: str
    prob: float
    win_rate: float
    show_rate: float


class RaceResult(BaseModel):
    id: str
    name: str
    distance: int
    time: str
    predictions: list[PredictionResult]


@app.post("/api/predict")
def predict(request: PredictRequest):
    """äºˆæ¸¬ã‚’å®Ÿè¡Œ"""
    track_code = request.track_code
    date_str = request.date.replace("-", "")

    if track_code not in TRACKS:
        raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰")

    model, model_features = load_model(track_code)
    if model is None:
        raise HTTPException(
            status_code=400,
            detail=f"{TRACKS[track_code]['name']}ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“"
        )

    scraper = NARScraper(track_code, delay=0.3)
    processor = Processor()

    # ãƒ¬ãƒ¼ã‚¹ä¸€è¦§å–å¾—
    race_ids = scraper.get_race_list_by_date(date_str)
    if not race_ids:
        return {"races": [], "message": "ãƒ¬ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}

    results = []
    for rid in sorted(race_ids):
        df = scraper.get_race_data(rid)
        if df is None:
            continue

        df = scraper.enrich_data(df)
        df = processor.process(df)

        # ã‚ªãƒƒã‚ºå–å¾—ï¼ˆå˜å‹ãƒ»è¤‡å‹ã‚’ä¸€æ‹¬å–å¾—ï¼‰
        all_odds = scraper.get_all_odds(rid)
        win_odds_dict = all_odds.get('win', {})
        place_odds_dict = all_odds.get('place', {})

        # äºˆæ¸¬
        X = df[model_features].fillna(-1)
        df['prob'] = model.predict(X)
        df['pred_rank'] = df['prob'].rank(ascending=False, method='min').astype(int)
        df = df.sort_values('prob', ascending=False)

        # ãƒ¬ãƒ¼ã‚¹ç•ªå·æŠ½å‡º
        race_num = rid[-2:]
        race_name = df['race_name'].iloc[0] if 'race_name' in df.columns else f"{race_num}R"
        distance = int(df['distance'].iloc[0]) if 'distance' in df.columns else 0
        start_time = df['start_time'].iloc[0] if 'start_time' in df.columns else ""

        predictions = []
        for i, (_, row) in enumerate(df.iterrows()):  # å…¨é¦¬ã‚’è¿”ã™
            horse_num = int(row['horse_number']) if pd.notna(row.get('horse_number')) else 0
            win_odds = win_odds_dict.get(horse_num, 0)
            place_odds_data = place_odds_dict.get(horse_num, {})
            place_odds = place_odds_data.get('avg', 0) if place_odds_data else 0
            place_odds_min = place_odds_data.get('min', 0) if place_odds_data else 0
            place_odds_max = place_odds_data.get('max', 0) if place_odds_data else 0
            prob = float(row['prob'])

            # å‹ç‡ãƒ»è¤‡å‹ç‡ã‚’å–å¾—ï¼ˆ0-1ã®ç¯„å›²ã§ã‚ã‚‹ã¹ãï¼‰
            raw_win_rate = float(row.get('horse_win_rate') or 0)
            raw_show_rate = float(row.get('horse_show_rate') or 0)

            win_rate = raw_win_rate * 100
            show_rate = raw_show_rate * 100

            # æœŸå¾…å€¤è¨ˆç®—ï¼ˆè¤‡å‹ã‚ªãƒƒã‚º Ã— AIç¢ºç‡ï¼‰
            # è¤‡å‹ã‚ªãƒƒã‚ºãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°å˜å‹/3ã§æ¨å®š
            effective_place_odds = place_odds if place_odds > 0 else (win_odds / 3 if win_odds > 0 else 0)
            expected_value = prob * effective_place_odds if effective_place_odds > 0 else 0

            # å¦™å‘³åˆ¤å®š: æœŸå¾…å€¤ > 1.0 ãªã‚‰é»’å­—æœŸå¾…
            is_value = expected_value > 1.0

            predictions.append({
                "rank": i + 1,
                "number": horse_num,
                "name": row.get('horse_name', 'ä¸æ˜'),
                "jockey": row.get('jockey_name', 'ä¸æ˜'),
                "prob": round(prob, 3),
                "win_rate": round(win_rate, 1),
                "show_rate": round(show_rate, 1),
                "odds": win_odds,
                "place_odds": place_odds,
                "place_odds_min": place_odds_min,
                "place_odds_max": place_odds_max,
                "expected_value": round(expected_value, 2),
                "is_value": is_value
            })

        results.append({
            "id": race_num,
            "name": race_name,
            "distance": distance,
            "time": start_time,
            "field_size": len(df),  # å‡ºèµ°é ­æ•°ã‚’è¿½åŠ 
            "predictions": predictions
        })

    return {
        "track": {
            "code": track_code,
            "name": TRACKS[track_code]['name'],
            "emoji": TRACKS[track_code]['emoji']
        },
        "date": request.date,
        "races": results
    }


class RaceListRequest(BaseModel):
    track_code: str
    date: str


@app.post("/api/races")
def get_race_list(request: RaceListRequest):
    """ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—ï¼ˆè»½é‡ï¼‰"""
    track_code = request.track_code
    date_str = request.date.replace("-", "")

    if track_code not in TRACKS:
        raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰")

    scraper = NARScraper(track_code, delay=0.3)
    race_ids = scraper.get_race_list_by_date(date_str)

    return {
        "track": TRACKS[track_code],
        "race_ids": sorted(race_ids)
    }


class SingleRaceRequest(BaseModel):
    race_id: str
    track_code: str


@app.post("/api/predict/race")
def predict_single_race(request: SingleRaceRequest):
    """å˜ä¸€ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬"""
    race_id = request.race_id
    track_code = request.track_code

    if track_code not in TRACKS:
        raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰")

    model, model_features = load_model(track_code)
    if model is None:
        raise HTTPException(
            status_code=400,
            detail=f"{TRACKS[track_code]['name']}ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“"
        )

    scraper = NARScraper(track_code, delay=0.3)
    processor = Processor()

    df = scraper.get_race_data(race_id)
    if df is None:
        raise HTTPException(status_code=404, detail="ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“")

    df = scraper.enrich_data(df)
    df = processor.process(df)

    # ã‚ªãƒƒã‚ºå–å¾—ï¼ˆå˜å‹ãƒ»è¤‡å‹ã‚’ä¸€æ‹¬å–å¾—ï¼‰
    all_odds = scraper.get_all_odds(race_id)
    win_odds_dict = all_odds.get('win', {})
    place_odds_dict = all_odds.get('place', {})

    # äºˆæ¸¬
    X = df[model_features].fillna(-1)
    df['prob'] = model.predict(X)
    df['pred_rank'] = df['prob'].rank(ascending=False, method='min').astype(int)
    df = df.sort_values('prob', ascending=False)

    # ãƒ¬ãƒ¼ã‚¹æƒ…å ±
    race_num = race_id[-2:]
    race_name = df['race_name'].iloc[0] if 'race_name' in df.columns else f"{race_num}R"
    distance = int(df['distance'].iloc[0]) if 'distance' in df.columns else 0
    start_time = df['start_time'].iloc[0] if 'start_time' in df.columns else ""

    predictions = []
    for i, (_, row) in enumerate(df.iterrows()):  # å…¨é¦¬ã‚’è¿”ã™
        horse_num = int(row['horse_number']) if pd.notna(row.get('horse_number')) else 0
        win_odds = win_odds_dict.get(horse_num, 0)
        place_odds_data = place_odds_dict.get(horse_num, {})
        place_odds = place_odds_data.get('avg', 0) if place_odds_data else 0
        place_odds_min = place_odds_data.get('min', 0) if place_odds_data else 0
        place_odds_max = place_odds_data.get('max', 0) if place_odds_data else 0
        prob = float(row['prob'])

        # å‹ç‡ãƒ»è¤‡å‹ç‡ã‚’å–å¾—ï¼ˆ0-1ã®ç¯„å›²ã§ã‚ã‚‹ã¹ãï¼‰
        raw_win_rate = float(row.get('horse_win_rate') or 0)
        raw_show_rate = float(row.get('horse_show_rate') or 0)

        win_rate = raw_win_rate * 100
        show_rate = raw_show_rate * 100

        # æœŸå¾…å€¤è¨ˆç®—ï¼ˆè¤‡å‹ã‚ªãƒƒã‚º Ã— AIç¢ºç‡ï¼‰
        effective_place_odds = place_odds if place_odds > 0 else (win_odds / 3 if win_odds > 0 else 0)
        expected_value = prob * effective_place_odds if effective_place_odds > 0 else 0
        is_value = expected_value > 1.0

        predictions.append({
            "rank": i + 1,
            "number": horse_num,
            "name": row.get('horse_name', 'ä¸æ˜'),
            "jockey": row.get('jockey_name', 'ä¸æ˜'),
            "prob": round(prob, 3),
            "win_rate": round(win_rate, 1),
            "show_rate": round(show_rate, 1),
            "odds": win_odds,
            "place_odds": place_odds,
            "place_odds_min": place_odds_min,
            "place_odds_max": place_odds_max,
            "expected_value": round(expected_value, 2),
            "is_value": is_value
        })

    # äºˆæ¸¬ãƒ­ã‚°ã‚’ä¿å­˜ï¼ˆèª¤ç­”åˆ†æç”¨ï¼‰
    metadata = {
        "race_name": race_name,
        "distance": distance,
        "track_condition": df['track_condition'].iloc[0] if 'track_condition' in df.columns else "ä¸æ˜",
        "weather": df['weather'].iloc[0] if 'weather' in df.columns else "ä¸æ˜",
        "field_size": len(df)
    }
    save_prediction_log(race_id, track_code, predictions, metadata)

    return {
        "id": race_num,
        "name": race_name,
        "distance": distance,
        "time": start_time,
        "field_size": len(df),
        "predictions": predictions
    }


# ========== è»½é‡ã‚ªãƒƒã‚ºå–å¾—API ==========

class OddsRequest(BaseModel):
    race_id: str
    track_code: str


def get_race_result(race_id: str) -> list:
    """ãƒ¬ãƒ¼ã‚¹çµæœï¼ˆç€é †ï¼‰ã‚’å–å¾—"""
    url = f"https://nar.netkeiba.com/race/result.html?race_id={race_id}"
    try:
        time.sleep(0.2)
        session = requests.Session()
        session.headers.update({'User-Agent': 'Mozilla/5.0'})
        r = session.get(url, timeout=10)
        r.encoding = 'EUC-JP'
        soup = BeautifulSoup(r.text, 'lxml')

        results = []
        table = soup.find('table', class_='RaceTable01')
        if not table:
            table = soup.find('table', class_='Result_Table')
        if not table:
            return []

        for tr in table.find_all('tr'):
            tds = tr.find_all('td')
            if len(tds) < 3:
                continue

            rank_text = tds[0].get_text(strip=True)
            if not rank_text.isdigit():
                continue
            rank = int(rank_text)

            # é¦¬ç•ªã‚’å–å¾—ï¼ˆtds[2]ãŒé¦¬ç•ªã€tds[1]ã¯æ ç•ªï¼‰
            horse_num = None
            if len(tds) >= 3:
                umaban_text = tds[2].get_text(strip=True)
                if umaban_text.isdigit() and 1 <= int(umaban_text) <= 18:
                    horse_num = int(umaban_text)

            if horse_num:
                results.append({"rank": rank, "number": horse_num})

        return sorted(results, key=lambda x: x["rank"])[:3]  # TOP3ã®ã¿
    except:
        return []


@app.post("/api/odds")
def get_odds_only(request: OddsRequest):
    """ã‚ªãƒƒã‚ºã¨çµæœã‚’å–å¾—ï¼ˆãƒ¬ãƒ¼ã‚¹çµ‚äº†æ™‚ã¯çµæœã‚‚å«ã‚€ï¼‰"""
    race_id = request.race_id
    track_code = request.track_code

    if track_code not in TRACKS:
        raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰")

    scraper = NARScraper(track_code, delay=0.2)
    odds_dict = scraper.get_odds(race_id)

    # çµæœã‚‚å–å¾—ï¼ˆçµ‚äº†ã—ã¦ã„ã‚Œã°è¿”ã‚‹ã€ã¾ã ãªã‚‰ç©ºï¼‰
    result = get_race_result(race_id)

    return {
        "race_id": race_id,
        "odds": odds_dict,
        "result": result if result else None  # çµ‚äº†ã—ã¦ã„ãªã‘ã‚Œã°null
    }


# ========== äº‹å‰è¨ˆç®—æ¸ˆã¿äºˆæ¸¬å–å¾—API ==========

@app.get("/api/predictions/{date}/{track_code}")
def get_precomputed_predictions(date: str, track_code: str):
    """äº‹å‰è¨ˆç®—æ¸ˆã¿ã®äºˆæ¸¬JSONã‚’å–å¾—"""
    if track_code not in TRACKS:
        raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰")

    predictions_file = BASE_DIR / "predictions" / date / f"{track_code}.json"

    if not predictions_file.exists():
        raise HTTPException(status_code=404, detail="äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    import json
    with open(predictions_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    return data


@app.get("/api/predictions/{date}")
def list_available_predictions(date: str):
    """æŒ‡å®šæ—¥ã®åˆ©ç”¨å¯èƒ½ãªäºˆæ¸¬ä¸€è¦§"""
    predictions_dir = BASE_DIR / "predictions" / date

    if not predictions_dir.exists():
        return {"date": date, "tracks": []}

    available = []
    for f in predictions_dir.glob("*.json"):
        track_code = f.stem
        if track_code in TRACKS:
            available.append({
                "code": track_code,
                "name": TRACKS[track_code]['name'],
                "emoji": TRACKS[track_code]['emoji']
            })

    return {"date": date, "tracks": available}


# ========== ç²¾åº¦è©•ä¾¡API ==========

@app.get("/api/accuracy/{date}/{track_code}")
def get_accuracy(date: str, track_code: str):
    """æŒ‡å®šæ—¥ãƒ»ç«¶é¦¬å ´ã®ç²¾åº¦ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    if track_code not in TRACKS:
        raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰")

    accuracy_file = BASE_DIR / "accuracy" / date / f"{track_code}.json"

    if not accuracy_file.exists():
        raise HTTPException(status_code=404, detail="ç²¾åº¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    import json
    with open(accuracy_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    return data


@app.get("/api/accuracy/{date}")
def get_daily_accuracy(date: str):
    """æŒ‡å®šæ—¥ã®å…¨ä½“ç²¾åº¦ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
    summary_file = BASE_DIR / "accuracy" / date / "summary.json"

    if not summary_file.exists():
        raise HTTPException(status_code=404, detail="ç²¾åº¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    import json
    with open(summary_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    return data


@app.get("/api/accuracy")
def get_accuracy_history():
    """éå»ã®ç²¾åº¦ãƒ‡ãƒ¼ã‚¿ä¸€è¦§ã‚’å–å¾—"""
    accuracy_dir = BASE_DIR / "accuracy"

    if not accuracy_dir.exists():
        return {"dates": []}

    dates = []
    for d in sorted(accuracy_dir.iterdir(), reverse=True):
        if d.is_dir():
            summary_file = d / "summary.json"
            if summary_file.exists():
                import json
                with open(summary_file, 'r', encoding='utf-8') as f:
                    summary = json.load(f)
                dates.append(summary)

    return {"history": dates[:30]}  # ç›´è¿‘30æ—¥åˆ†


# ========== ãƒ¢ãƒ‡ãƒ«æƒ…å ±API ==========

@app.get("/api/models/{track_code}")
def get_model_info(track_code: str):
    """ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
    if track_code not in TRACKS:
        raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰")

    model_path = TRACKS[track_code]['model']
    meta_path = model_path.replace('.pkl', '_meta.json')
    meta_file = BASE_DIR / meta_path

    if meta_file.exists():
        import json
        with open(meta_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONãŒãªã„å ´åˆã€pklã‹ã‚‰èª­ã¿è¾¼ã¿è©¦è¡Œ
    model_file = BASE_DIR / model_path
    if model_file.exists():
        try:
            with open(model_file, 'rb') as f:
                data = pickle.load(f)
            if 'metadata' in data:
                return data['metadata']
        except:
            pass

    raise HTTPException(status_code=404, detail="ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“")


@app.get("/api/models")
def get_all_models_info():
    """å…¨ãƒ¢ãƒ‡ãƒ«ã®æƒ…å ±ã‚’å–å¾—"""
    models_info = []

    for code, info in TRACKS.items():
        model_path = info['model']
        meta_path = model_path.replace('.pkl', '_meta.json')
        meta_file = BASE_DIR / meta_path
        model_file = BASE_DIR / model_path

        model_data = {
            "code": code,
            "name": info['name'],
            "emoji": info['emoji'],
            "model_exists": model_file.exists(),
            "metadata": None
        }

        if meta_file.exists():
            try:
                import json
                with open(meta_file, 'r', encoding='utf-8') as f:
                    model_data["metadata"] = json.load(f)
            except:
                pass

        models_info.append(model_data)

    return {"models": models_info}


# ========== èª¤ç­”åˆ†æAPIï¼ˆSSEå¯¾å¿œï¼‰ ==========

def analyze_get_race_result(race_id: str) -> list:
    """ãƒ¬ãƒ¼ã‚¹çµæœï¼ˆç€é †ï¼‰ã‚’å–å¾—ï¼ˆåˆ†æç”¨ï¼‰"""
    url = f"https://nar.netkeiba.com/race/result.html?race_id={race_id}"
    try:
        session = requests.Session()
        session.headers.update({'User-Agent': 'Mozilla/5.0'})
        r = session.get(url, timeout=10)
        r.encoding = 'EUC-JP'
        soup = BeautifulSoup(r.text, 'lxml')

        results = []
        table = soup.find('table', class_='RaceTable01')
        if not table:
            table = soup.find('table', class_='Result_Table')
        if not table:
            return []

        for tr in table.find_all('tr'):
            tds = tr.find_all('td')
            if len(tds) < 3:
                continue

            rank_text = tds[0].get_text(strip=True)
            if not rank_text.isdigit():
                continue
            rank = int(rank_text)

            umaban_text = tds[2].get_text(strip=True)
            if umaban_text.isdigit() and 1 <= int(umaban_text) <= 18:
                horse_num = int(umaban_text)
                results.append({"rank": rank, "number": horse_num})

        return sorted(results, key=lambda x: x["rank"])
    except Exception as e:
        print(f"Error fetching result for {race_id}: {e}")
        return []


def compare_prediction(prediction_log: dict, result: list) -> dict:
    """äºˆæ¸¬ã¨çµæœã‚’ç…§åˆ"""
    if not result:
        return None

    predictions = prediction_log["predictions"]
    metadata = prediction_log.get("metadata", {})

    pred_top3 = [p["number"] for p in predictions[:3]]
    pred_1st = predictions[0]["number"] if predictions else None
    actual_top3 = [r["number"] for r in result[:3]]
    actual_1st = result[0]["number"] if result else None

    win_hit = (pred_1st == actual_1st)
    show_hit = (pred_1st in actual_top3)

    # äºˆæ¸¬1ä½ã®é¦¬ãŒå®Ÿéš›ã«ä½•ç€ã ã£ãŸã‹
    pred_1st_actual_rank = None
    for r in result:
        if r["number"] == pred_1st:
            pred_1st_actual_rank = r["rank"]
            break

    # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—åˆ†é¡
    error_type = None
    if not show_hit:
        if pred_1st_actual_rank is None:
            error_type = "å‡ºèµ°å–æ¶ˆ"
        elif pred_1st_actual_rank >= 10:
            error_type = "å¤§å¤–ã‚Œ(10ç€ä»¥ä¸‹)"
        elif pred_1st_actual_rank >= 6:
            error_type = "ä¸­å¤–ã‚Œ(6-9ç€)"
        elif pred_1st_actual_rank >= 4:
            error_type = "æƒœã—ã„(4-5ç€)"

    return {
        "race_id": prediction_log["race_id"],
        "track_name": prediction_log.get("track_name", "ä¸æ˜"),
        "race_name": metadata.get("race_name", "ä¸æ˜"),
        "pred_1st": pred_1st,
        "actual_1st": actual_1st,
        "win_hit": win_hit,
        "show_hit": show_hit,
        "pred_1st_actual_rank": pred_1st_actual_rank,
        "error_type": error_type,
        "metadata": metadata
    }


async def analyze_stream(date: str):
    """åˆ†æã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å®Ÿè¡Œ"""
    log_dir = BASE_DIR / "prediction_logs" / date

    if not log_dir.exists():
        yield f"data: {json.dumps({'type': 'error', 'message': 'äºˆæ¸¬ãƒ­ã‚°ãŒã‚ã‚Šã¾ã›ã‚“'})}\n\n"
        return

    log_files = list(log_dir.glob("*.json"))
    total = len(log_files)

    if total == 0:
        yield f"data: {json.dumps({'type': 'error', 'message': 'äºˆæ¸¬ãƒ­ã‚°ãŒã‚ã‚Šã¾ã›ã‚“'})}\n\n"
        return

    yield f"data: {json.dumps({'type': 'start', 'total': total})}\n\n"

    comparisons = []
    for i, log_file in enumerate(log_files):
        with open(log_file, 'r', encoding='utf-8') as f:
            prediction_log = json.load(f)

        race_id = prediction_log["race_id"]

        # é€²æ—ã‚’é€ä¿¡
        yield f"data: {json.dumps({'type': 'progress', 'current': i + 1, 'total': total, 'race_id': race_id})}\n\n"

        # çµæœã‚’å–å¾—
        result = analyze_get_race_result(race_id)
        if result:
            comparison = compare_prediction(prediction_log, result)
            if comparison:
                comparisons.append(comparison)

        # å°‘ã—å¾…æ©Ÿï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰
        await asyncio.sleep(0.3)

    # é›†è¨ˆ
    if not comparisons:
        yield f"data: {json.dumps({'type': 'error', 'message': 'ç…§åˆã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“'})}\n\n"
        return

    # çµ±è¨ˆã‚’è¨ˆç®—
    total_races = len(comparisons)
    win_hits = sum(1 for c in comparisons if c["win_hit"])
    show_hits = sum(1 for c in comparisons if c["show_hit"])

    # é¦¬å ´çŠ¶æ…‹åˆ¥
    by_track_condition = defaultdict(lambda: {"total": 0, "show_hits": 0})
    # å¤©æ°—åˆ¥
    by_weather = defaultdict(lambda: {"total": 0, "show_hits": 0})
    # è·é›¢åˆ¥
    by_distance = defaultdict(lambda: {"total": 0, "show_hits": 0})
    # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—
    error_types = defaultdict(int)

    for c in comparisons:
        meta = c.get("metadata", {})

        # é¦¬å ´çŠ¶æ…‹
        track_cond = meta.get("track_condition", "ä¸æ˜")
        by_track_condition[track_cond]["total"] += 1
        if c["show_hit"]:
            by_track_condition[track_cond]["show_hits"] += 1

        # å¤©æ°—
        weather = meta.get("weather", "ä¸æ˜")
        by_weather[weather]["total"] += 1
        if c["show_hit"]:
            by_weather[weather]["show_hits"] += 1

        # è·é›¢
        distance = meta.get("distance", 0)
        if distance < 1400:
            dist_cat = "çŸ­è·é›¢(<1400m)"
        elif distance < 1800:
            dist_cat = "ä¸­è·é›¢(1400-1800m)"
        else:
            dist_cat = "é•·è·é›¢(>1800m)"
        by_distance[dist_cat]["total"] += 1
        if c["show_hit"]:
            by_distance[dist_cat]["show_hits"] += 1

        # ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—
        if c.get("error_type"):
            error_types[c["error_type"]] += 1

    # çµæœã‚’é€ä¿¡
    result_data = {
        "type": "result",
        "date": date,
        "summary": {
            "total_races": total_races,
            "win_hits": win_hits,
            "win_rate": round(win_hits / total_races * 100, 1) if total_races > 0 else 0,
            "show_hits": show_hits,
            "show_rate": round(show_hits / total_races * 100, 1) if total_races > 0 else 0
        },
        "by_track_condition": {k: v for k, v in by_track_condition.items()},
        "by_weather": {k: v for k, v in by_weather.items()},
        "by_distance": {k: v for k, v in by_distance.items()},
        "error_types": dict(error_types),
        "details": comparisons
    }

    yield f"data: {json.dumps(result_data, ensure_ascii=False)}\n\n"

    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_dir = BASE_DIR / "analysis_reports" / date
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / "report.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)

    yield f"data: {json.dumps({'type': 'complete', 'saved_to': str(output_file)})}\n\n"


@app.get("/api/analyze/{date}")
def analyze_predictions(date: str):
    """äºˆæ¸¬ã®èª¤ç­”åˆ†æã‚’å®Ÿè¡Œï¼ˆé€šå¸¸APIç‰ˆï¼‰"""
    log_dir = BASE_DIR / "prediction_logs" / date

    if not log_dir.exists():
        raise HTTPException(status_code=404, detail="äºˆæ¸¬ãƒ­ã‚°ãŒã‚ã‚Šã¾ã›ã‚“")

    log_files = list(log_dir.glob("*.json"))
    total = len(log_files)

    if total == 0:
        raise HTTPException(status_code=404, detail="äºˆæ¸¬ãƒ­ã‚°ãŒã‚ã‚Šã¾ã›ã‚“")

    comparisons = []
    for log_file in log_files:
        with open(log_file, 'r', encoding='utf-8') as f:
            prediction_log = json.load(f)

        race_id = prediction_log["race_id"]

        # çµæœã‚’å–å¾—
        result = analyze_get_race_result(race_id)
        if result:
            comparison = compare_prediction(prediction_log, result)
            if comparison:
                comparisons.append(comparison)

        # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
        time.sleep(0.3)

    if not comparisons:
        raise HTTPException(status_code=404, detail="ç…§åˆã§ãã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    # çµ±è¨ˆã‚’è¨ˆç®—
    total_races = len(comparisons)
    win_hits = sum(1 for c in comparisons if c["win_hit"])
    show_hits = sum(1 for c in comparisons if c["show_hit"])

    # é¦¬å ´çŠ¶æ…‹åˆ¥
    by_track_condition = defaultdict(lambda: {"total": 0, "show_hits": 0})
    by_weather = defaultdict(lambda: {"total": 0, "show_hits": 0})
    by_distance = defaultdict(lambda: {"total": 0, "show_hits": 0})
    error_types = defaultdict(int)

    for c in comparisons:
        meta = c.get("metadata", {})

        track_cond = meta.get("track_condition", "ä¸æ˜")
        by_track_condition[track_cond]["total"] += 1
        if c["show_hit"]:
            by_track_condition[track_cond]["show_hits"] += 1

        weather = meta.get("weather", "ä¸æ˜")
        by_weather[weather]["total"] += 1
        if c["show_hit"]:
            by_weather[weather]["show_hits"] += 1

        distance = meta.get("distance", 0)
        if distance < 1400:
            dist_cat = "çŸ­è·é›¢(<1400m)"
        elif distance < 1800:
            dist_cat = "ä¸­è·é›¢(1400-1800m)"
        else:
            dist_cat = "é•·è·é›¢(>1800m)"
        by_distance[dist_cat]["total"] += 1
        if c["show_hit"]:
            by_distance[dist_cat]["show_hits"] += 1

        if c.get("error_type"):
            error_types[c["error_type"]] += 1

    result_data = {
        "date": date,
        "summary": {
            "total_races": total_races,
            "win_hits": win_hits,
            "win_rate": round(win_hits / total_races * 100, 1) if total_races > 0 else 0,
            "show_hits": show_hits,
            "show_rate": round(show_hits / total_races * 100, 1) if total_races > 0 else 0
        },
        "by_track_condition": {k: v for k, v in by_track_condition.items()},
        "by_weather": {k: v for k, v in by_weather.items()},
        "by_distance": {k: v for k, v in by_distance.items()},
        "error_types": dict(error_types),
        "details": comparisons
    }

    # çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_dir = BASE_DIR / "analysis_reports" / date
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / "report.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, ensure_ascii=False, indent=2)

    return result_data


@app.get("/api/analysis/{date}")
def get_analysis_report(date: str):
    """ä¿å­˜æ¸ˆã¿ã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—"""
    report_file = BASE_DIR / "analysis_reports" / date / "report.json"

    if not report_file.exists():
        raise HTTPException(status_code=404, detail="åˆ†æãƒ¬ãƒãƒ¼ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")

    with open(report_file, 'r', encoding='utf-8') as f:
        return json.load(f)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
