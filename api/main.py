# åœ°æ–¹ç«¶é¦¬ äºˆæ¸¬API
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional
import pandas as pd
import numpy as np
import pickle
import requests
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime
import os
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
BASE_DIR = Path(__file__).resolve().parent.parent

app = FastAPI(
    title="åœ°æ–¹ç«¶é¦¬äºˆæ¸¬API",
    description="AIãŒäºˆæ¸¬ã™ã‚‹åœ°æ–¹ç«¶é¦¬ã®3ç€ä»¥å†…äºˆæ¸¬",
    version="1.0.0"
)

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æœ¬ç•ªç’°å¢ƒã§ã¯é©åˆ‡ã«åˆ¶é™
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ========== ç«¶é¦¬å ´è¨­å®š ==========
TRACKS = {
    "44": {"name": "å¤§äº•", "model": "models/model_ohi.pkl", "emoji": "ğŸŸï¸"},
    "45": {"name": "å·å´", "model": "models/model_kawasaki.pkl", "emoji": "ğŸŒŠ"},
    "43": {"name": "èˆ¹æ©‹", "model": "models/model_funabashi.pkl", "emoji": "âš“"},
    "42": {"name": "æµ¦å’Œ", "model": "models/model_urawa.pkl", "emoji": "ğŸŒ¸"},
    "30": {"name": "é–€åˆ¥", "model": "models/model_monbetsu.pkl", "emoji": "ğŸ´"},
    "35": {"name": "ç››å²¡", "model": "models/model_morioka.pkl", "emoji": "â›°ï¸"},
    "36": {"name": "æ°´æ²¢", "model": "models/model_mizusawa.pkl", "emoji": "ğŸ’§"},
    "46": {"name": "é‡‘æ²¢", "model": "models/model_kanazawa.pkl", "emoji": "âœ¨"},
    "47": {"name": "ç¬ æ¾", "model": "models/model_kasamatsu.pkl", "emoji": "ğŸ‹"},
    "48": {"name": "åå¤å±‹", "model": "models/model_nagoya.pkl", "emoji": "ğŸ¯"},
    "50": {"name": "åœ’ç”°", "model": "models/model_sonoda.pkl", "emoji": "ğŸŒ³"},
    "51": {"name": "å§«è·¯", "model": "models/model_himeji.pkl", "emoji": "ğŸ°"},
    "54": {"name": "é«˜çŸ¥", "model": "models/model_kochi.pkl", "emoji": "ğŸ‹"},
    "55": {"name": "ä½è³€", "model": "models/model_saga.pkl", "emoji": "ğŸ‹"},
}

# ãƒ¢ãƒ‡ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥
model_cache = {}

# æ—§ãƒ¢ãƒ‡ãƒ«åã¨ã®äº’æ›æ€§
MODEL_ALIASES = {
    "models/model_ohi.pkl": ["models/model_ohi.pkl", "model_v2.pkl"],
}


# ========== ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ ==========
class NARScraper:
    BASE_URL = "https://nar.netkeiba.com"
    DB_URL = "https://db.netkeiba.com"

    def __init__(self, track_code, delay=0.5):
        self.track_code = track_code
        self.delay = delay
        self.session = requests.Session()
        self.session.headers.update({'User-Agent': 'Mozilla/5.0'})
        self.horse_cache = {}
        self.jockey_cache = {}

    def _fetch(self, url, encoding='EUC-JP'):
        time.sleep(self.delay)
        r = self.session.get(url)
        r.encoding = encoding
        return BeautifulSoup(r.text, 'lxml')

    def get_race_list_by_date(self, date: str) -> list:
        url = f"{self.BASE_URL}/top/race_list_sub.html?kaisai_date={date}"
        try:
            soup = self._fetch(url, encoding='UTF-8')
            ids = []
            for a in soup.find_all('a', href=True):
                m = re.search(r'race_id=(\d+)', a['href'])
                if m:
                    race_id = m.group(1)
                    if len(race_id) >= 6 and race_id[4:6] == self.track_code:
                        ids.append(race_id)
            return list(set(ids))
        except:
            return []

    def get_race_data(self, race_id: str):
        url = f"{self.BASE_URL}/race/shutuba.html?race_id={race_id}"
        try:
            soup = self._fetch(url)
            info = {'race_id': race_id}

            # ãƒ¬ãƒ¼ã‚¹å
            nm = soup.find('h1', class_='RaceName')
            if nm:
                info['race_name'] = nm.get_text(strip=True)

            # ç™ºèµ°æ™‚åˆ»
            rd = soup.find('div', class_='RaceData01')
            if rd:
                text = rd.get_text()
                tm = re.search(r'(\d{1,2}):(\d{2})', text)
                if tm:
                    info['start_time'] = f"{tm.group(1)}:{tm.group(2)}"
                dm = re.search(r'(\d{3,4})m', text)
                if dm:
                    info['distance'] = int(dm.group(1))

            # ãƒ†ãƒ¼ãƒ–ãƒ«å–å¾—
            table = soup.find('table', class_='ShutubaTable')
            if not table:
                table = soup.find('table', class_='RaceTable01')
            if not table:
                for t in soup.find_all('table'):
                    if t.find('a', href=re.compile(r'/horse/')):
                        table = t
                        break
            if not table:
                return None

            rows = []
            for tr in table.find_all('tr'):
                tds = tr.find_all('td')
                if len(tds) < 4:
                    continue

                data = info.copy()

                bracket_text = tds[0].get_text(strip=True)
                if bracket_text.isdigit():
                    data['bracket'] = int(bracket_text)
                umaban_text = tds[1].get_text(strip=True)
                if umaban_text.isdigit():
                    data['horse_number'] = int(umaban_text)

                horse_link = tr.find('a', href=re.compile(r'/horse/\d+'))
                if horse_link:
                    data['horse_name'] = horse_link.get_text(strip=True)
                    m = re.search(r'/horse/(\d+)', horse_link['href'])
                    if m:
                        data['horse_id'] = m.group(1)

                jockey_link = tr.find('a', href=re.compile(r'/jockey/'))
                if jockey_link:
                    data['jockey_name'] = jockey_link.get_text(strip=True)
                    m = re.search(r'/jockey/(?:result/recent/)?([a-zA-Z0-9]+)', jockey_link['href'])
                    if m:
                        data['jockey_id'] = m.group(1)

                for td in tds:
                    text = td.get_text(strip=True)
                    if re.match(r'^[ç‰¡ç‰ã‚»]\d$', text):
                        data['sex'] = text[0]
                        data['age'] = int(text[1])
                    if re.match(r'^\d{2}(\.\d)?$', text):
                        w = float(text)
                        if 45 <= w <= 65 and 'weight_carried' not in data:
                            data['weight_carried'] = w

                if data.get('horse_name'):
                    rows.append(data)

            if not rows:
                return None

            df = pd.DataFrame(rows)
            df['field_size'] = len(df)
            return df
        except Exception as e:
            print(f'Error: {e}')

    def get_odds(self, race_id: str, horse_names: list = None) -> dict:
        """å˜å‹ã‚ªãƒƒã‚ºã‚’å–å¾—ï¼ˆè¤‡æ•°ã‚½ãƒ¼ã‚¹ã‹ã‚‰è©¦è¡Œï¼‰"""
        odds_dict = {}

        # 1. ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ï¼ˆé–‹å‚¬ä¸­ã®ãƒ¬ãƒ¼ã‚¹ç”¨ï¼‰
        url = f"{self.BASE_URL}/odds/odds_get_form.html?race_id={race_id}&type=b1"
        try:
            soup = self._fetch(url, encoding='UTF-8')

            # ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã®é¦¬åã‚’å–å¾—ã—ã¦ã€æ­£ã—ã„ãƒ¬ãƒ¼ã‚¹ã‹ç¢ºèª
            page_horse_names = []
            for tr in soup.find_all('tr'):
                horse_link = tr.find('a', href=re.compile(r'/horse/'))
                if horse_link:
                    page_horse_names.append(horse_link.get_text(strip=True))

            # é¦¬åãƒªã‚¹ãƒˆãŒæ¸¡ã•ã‚Œã¦ã„ã¦ã€ãƒšãƒ¼ã‚¸ã®é¦¬åã¨ä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
            is_correct_race = True
            if horse_names and page_horse_names:
                # å°‘ãªãã¨ã‚‚1é ­ãŒä¸€è‡´ã™ã‚‹ã‹ç¢ºèª
                is_correct_race = any(name in page_horse_names for name in horse_names[:3])

            if is_correct_race and page_horse_names:
                # ã‚ªãƒƒã‚ºãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—
                for tr in soup.find_all('tr'):
                    tds = tr.find_all('td')
                    if len(tds) >= 4:
                        try:
                            umaban_text = tds[1].get_text(strip=True)
                            if umaban_text.isdigit() and 1 <= int(umaban_text) <= 18:
                                umaban = int(umaban_text)
                                for td in reversed(tds):
                                    text = td.get_text(strip=True)
                                    odds_match = re.match(r'^(\d+\.?\d*)$', text)
                                    if odds_match:
                                        val = float(odds_match.group(1))
                                        if 1.0 <= val <= 500:
                                            odds_dict[umaban] = val
                                            break
                        except (ValueError, IndexError):
                            continue

                if odds_dict:
                    return odds_dict
        except Exception as e:
            print(f'Odds page error: {e}')

        # ã‚ªãƒƒã‚ºãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã§ããªã‹ã£ãŸå ´åˆã€ã‚¹ãƒãƒ›ç‰ˆçµæœãƒšãƒ¼ã‚¸ã‹ã‚‰å…¨é¦¬ã®ã‚ªãƒƒã‚ºã‚’å–å¾—
        # ã‚¹ãƒãƒ›ç‰ˆã¯çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã«å…¨é¦¬ã®å˜å‹ã‚ªãƒƒã‚ºãŒå«ã¾ã‚Œã¦ã„ã‚‹
        sp_result_url = f"https://nar.sp.netkeiba.com/race/race_result.html?race_id={race_id}"
        try:
            soup = self._fetch(sp_result_url, encoding='UTF-8')

            # ã‚¹ãƒãƒ›ç‰ˆã®çµæœãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã‚ªãƒƒã‚ºã‚’å–å¾—
            # ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã®å„è¡Œã‹ã‚‰é¦¬ç•ªã¨ã‚ªãƒƒã‚ºã‚’æŠ½å‡º
            for tr in soup.find_all('tr'):
                tds = tr.find_all('td')
                if len(tds) >= 8:
                    try:
                        # é¦¬ç•ªã‚’æ¢ã™ï¼ˆé€šå¸¸ã¯æœ€åˆã®æ–¹ã®tdï¼‰
                        umaban = None
                        odds_val = None

                        for i, td in enumerate(tds):
                            text = td.get_text(strip=True)
                            # é¦¬ç•ªï¼ˆ1-18ã®æ•°å­—ã€é€šå¸¸2æ¡ä»¥ä¸‹ï¼‰
                            if text.isdigit() and 1 <= int(text) <= 18 and umaban is None:
                                # ç€é †ã§ã¯ãªãé¦¬ç•ªã‹ã‚’ç¢ºèªï¼ˆç€é †ã¯1ã‹ã‚‰å§‹ã¾ã‚‹å°ã•ã„æ•°å­—ï¼‰
                                # classå±æ€§ã‚„dataå±æ€§ã§åˆ¤åˆ¥ã§ãã‚‹å ´åˆã‚‚ã‚ã‚‹
                                td_class = td.get('class', [])
                                if 'Umaban' in str(td_class) or i >= 1:
                                    umaban = int(text)

                        # ã‚ªãƒƒã‚ºã‚’æ¢ã™ï¼ˆå°æ•°ç‚¹ã‚’å«ã‚€æ•°å­—ï¼‰
                        for td in tds:
                            text = td.get_text(strip=True)
                            # ã‚ªãƒƒã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³: "1.5" or "29.8" ãªã©
                            odds_match = re.match(r'^(\d+\.\d+)$', text)
                            if odds_match:
                                val = float(odds_match.group(1))
                                if 1.0 <= val <= 999.9:
                                    odds_val = val
                                    break

                        if umaban and odds_val:
                            odds_dict[umaban] = odds_val

                    except (ValueError, IndexError):
                        continue

            if odds_dict:
                return odds_dict

        except Exception as e:
            print(f'SP result page error: {e}')

        # æœ€å¾Œã®æ‰‹æ®µ: PCç‰ˆçµæœãƒšãƒ¼ã‚¸ã®æ‰•æˆ»é‡‘ã‹ã‚‰å‹ã¡é¦¬ã®ã¿å–å¾—
        result_url = f"{self.BASE_URL}/race/result.html?race_id={race_id}"
        try:
            soup = self._fetch(result_url)
            payout_table = soup.find('table', class_='Payout_Detail_Table')
            if payout_table:
                for tr in payout_table.find_all('tr'):
                    th = tr.find('th')
                    if th and 'å˜å‹' in th.get_text():
                        tds = tr.find_all('td')
                        if len(tds) >= 2:
                            umaban_text = tds[0].get_text(strip=True)
                            payout_text = tds[1].get_text(strip=True)
                            if umaban_text.isdigit():
                                umaban = int(umaban_text)
                                payout_match = re.search(r'([\d,]+)', payout_text)
                                if payout_match:
                                    payout = int(payout_match.group(1).replace(',', ''))
                                    odds_dict[umaban] = payout / 100
            return odds_dict
        except Exception as e:
            print(f'Result page error: {e}')
            return {}

    def get_horse_history(self, horse_id: str):
        if horse_id in self.horse_cache:
            return self.horse_cache[horse_id]

        url = f"{self.DB_URL}/horse/ajax_horse_results.html?id={horse_id}"
        try:
            time.sleep(self.delay)
            r = self.session.get(url)
            r.encoding = 'EUC-JP'
            soup = BeautifulSoup(r.text, 'lxml')

            results = []
            for tr in soup.find_all('tr'):
                tds = tr.find_all('td')
                if len(tds) < 6:
                    continue
                for td in tds[3:7]:
                    t = td.get_text(strip=True)
                    if t.isdigit() and 1 <= int(t) <= 20:
                        results.append(int(t))
                        break
                if len(results) >= 20:
                    break

            stats = self._calc_stats(results)
            self.horse_cache[horse_id] = stats
            return stats
        except:
            return self._empty_stats()

    def get_jockey_stats(self, jockey_id: str):
        if jockey_id in self.jockey_cache:
            return self.jockey_cache[jockey_id]

        url = f"{self.DB_URL}/jockey/{jockey_id}/"
        try:
            soup = self._fetch(url)
            text = soup.get_text()
            stats = {'jockey_win_rate': 0, 'jockey_place_rate': 0, 'jockey_show_rate': 0}

            m = re.search(r'å‹ç‡[ï¼š:\s]*(\d+\.?\d*)', text)
            if m:
                stats['jockey_win_rate'] = float(m.group(1)) / 100
            m = re.search(r'é€£å¯¾ç‡[ï¼š:\s]*(\d+\.?\d*)', text)
            if m:
                stats['jockey_place_rate'] = float(m.group(1)) / 100
            m = re.search(r'è¤‡å‹ç‡[ï¼š:\s]*(\d+\.?\d*)', text)
            if m:
                stats['jockey_show_rate'] = float(m.group(1)) / 100

            self.jockey_cache[jockey_id] = stats
            return stats
        except:
            return {'jockey_win_rate': 0, 'jockey_place_rate': 0, 'jockey_show_rate': 0}

    def _calc_stats(self, ranks):
        if not ranks:
            return self._empty_stats()
        total = len(ranks)
        wins = sum(1 for r in ranks if r == 1)
        place = sum(1 for r in ranks if r <= 2)
        show = sum(1 for r in ranks if r <= 3)
        recent = ranks[:5]
        r_total = len(recent)
        return {
            'horse_runs': total,
            'horse_win_rate': wins / total,
            'horse_place_rate': place / total,
            'horse_show_rate': show / total,
            'horse_avg_rank': np.mean(ranks),
            'horse_recent_win_rate': sum(1 for r in recent if r == 1) / r_total if r_total else 0,
            'horse_recent_show_rate': sum(1 for r in recent if r <= 3) / r_total if r_total else 0,
            'horse_recent_avg_rank': np.mean(recent) if recent else 10,
            'last_rank': ranks[0] if ranks else 10
        }

    def _empty_stats(self):
        return {
            'horse_runs': 0, 'horse_win_rate': 0, 'horse_place_rate': 0,
            'horse_show_rate': 0, 'horse_avg_rank': 10,
            'horse_recent_win_rate': 0, 'horse_recent_show_rate': 0,
            'horse_recent_avg_rank': 10, 'last_rank': 10
        }

    def enrich_data(self, df):
        df = df.copy()
        if 'horse_id' in df.columns:
            horse_data = []
            for hid in df['horse_id'].dropna().unique():
                stats = self.get_horse_history(str(hid))
                stats['horse_id'] = hid
                horse_data.append(stats)
            if horse_data:
                hdf = pd.DataFrame(horse_data)
                df['horse_id'] = df['horse_id'].astype(str)
                hdf['horse_id'] = hdf['horse_id'].astype(str)
                df = df.merge(hdf, on='horse_id', how='left')

        if 'jockey_id' in df.columns:
            jockey_data = []
            for jid in df['jockey_id'].dropna().unique():
                stats = self.get_jockey_stats(str(jid))
                stats['jockey_id'] = jid
                jockey_data.append(stats)
            if jockey_data:
                jdf = pd.DataFrame(jockey_data)
                df['jockey_id'] = df['jockey_id'].astype(str)
                jdf['jockey_id'] = jdf['jockey_id'].astype(str)
                df = df.merge(jdf, on='jockey_id', how='left')
        return df


# ========== å‰å‡¦ç† ==========
class Processor:
    def __init__(self):
        self.features = [
            'horse_runs', 'horse_win_rate', 'horse_place_rate', 'horse_show_rate',
            'horse_avg_rank', 'horse_recent_win_rate', 'horse_recent_show_rate',
            'horse_recent_avg_rank', 'last_rank',
            'jockey_win_rate', 'jockey_place_rate', 'jockey_show_rate',
            'horse_number', 'bracket', 'age', 'weight_carried', 'distance',
            'sex_encoded', 'track_encoded', 'field_size', 'weight_diff'
        ]

    def process(self, df):
        df = df.copy()
        num_cols = ['bracket', 'horse_number', 'age', 'weight_carried', 'distance',
                    'field_size', 'horse_runs', 'horse_win_rate', 'horse_place_rate',
                    'horse_show_rate', 'horse_avg_rank', 'horse_recent_win_rate',
                    'horse_recent_show_rate', 'horse_recent_avg_rank', 'last_rank',
                    'jockey_win_rate', 'jockey_place_rate', 'jockey_show_rate']
        for c in num_cols:
            if c in df.columns:
                df[c] = pd.to_numeric(df[c], errors='coerce')

        if 'sex' in df.columns:
            df['sex_encoded'] = df['sex'].map({'ç‰¡': 0, 'ç‰': 1, 'ã‚»': 2}).fillna(0)
        else:
            df['sex_encoded'] = 0

        df['track_encoded'] = 0

        if 'weight_carried' in df.columns and 'race_id' in df.columns:
            df['weight_diff'] = df.groupby('race_id')['weight_carried'].transform(lambda x: x - x.mean())
        else:
            df['weight_diff'] = 0

        if 'field_size' not in df.columns:
            if 'race_id' in df.columns:
                df['field_size'] = df.groupby('race_id')['race_id'].transform('count')
            else:
                df['field_size'] = 12

        for f in self.features:
            if f not in df.columns:
                df[f] = 0
        return df

    def get_features(self):
        return self.features


# ========== ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ ==========
def load_model(track_code: str):
    if track_code in model_cache:
        return model_cache[track_code]

    if track_code not in TRACKS:
        return None, None

    model_name = TRACKS[track_code]['model']

    # ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæ—§ãƒ¢ãƒ‡ãƒ«åã¨ã®äº’æ›æ€§ï¼‰
    paths_to_try = [model_name]
    if model_name in MODEL_ALIASES:
        paths_to_try = MODEL_ALIASES[model_name]

    for model_name in paths_to_try:
        model_path = BASE_DIR / model_name
        if model_path.exists():
            with open(model_path, 'rb') as f:
                d = pickle.load(f)
            model_cache[track_code] = (d['model'], d['features'])
            return d['model'], d['features']
    return None, None


# ========== APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ==========

@app.get("/")
def root():
    return {"message": "åœ°æ–¹ç«¶é¦¬äºˆæ¸¬API", "version": "1.0.0"}


@app.get("/api/tracks")
def get_tracks():
    """åˆ©ç”¨å¯èƒ½ãªç«¶é¦¬å ´ä¸€è¦§ã‚’å–å¾—"""
    tracks = []
    for code, info in TRACKS.items():
        model_name = info['model']
        # ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚‚å«ã‚ã¦ãƒã‚§ãƒƒã‚¯
        paths_to_check = [model_name]
        if model_name in MODEL_ALIASES:
            paths_to_check = MODEL_ALIASES[model_name]
        model_exists = any((BASE_DIR / p).exists() for p in paths_to_check)
        tracks.append({
            "code": code,
            "name": info['name'],
            "emoji": info['emoji'],
            "model_available": model_exists
        })
    return {"tracks": tracks}


class PredictRequest(BaseModel):
    track_code: str
    date: str  # YYYY-MM-DDå½¢å¼


class PredictionResult(BaseModel):
    rank: int
    number: int
    name: str
    jockey: str
    prob: float
    win_rate: float
    show_rate: float


class RaceResult(BaseModel):
    id: str
    name: str
    distance: int
    time: str
    predictions: list[PredictionResult]


@app.post("/api/predict")
def predict(request: PredictRequest):
    """äºˆæ¸¬ã‚’å®Ÿè¡Œ"""
    track_code = request.track_code
    date_str = request.date.replace("-", "")

    if track_code not in TRACKS:
        raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰")

    model, model_features = load_model(track_code)
    if model is None:
        raise HTTPException(
            status_code=400,
            detail=f"{TRACKS[track_code]['name']}ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“"
        )

    scraper = NARScraper(track_code, delay=0.3)
    processor = Processor()

    # ãƒ¬ãƒ¼ã‚¹ä¸€è¦§å–å¾—
    race_ids = scraper.get_race_list_by_date(date_str)
    if not race_ids:
        return {"races": [], "message": "ãƒ¬ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}

    results = []
    for rid in sorted(race_ids):
        df = scraper.get_race_data(rid)
        if df is None:
            continue

        df = scraper.enrich_data(df)
        df = processor.process(df)

        # ã‚ªãƒƒã‚ºå–å¾—ï¼ˆé¦¬åãƒªã‚¹ãƒˆã‚’æ¸¡ã—ã¦æ­£ã—ã„ãƒ¬ãƒ¼ã‚¹ã‹ç¢ºèªï¼‰
        horse_names = df['horse_name'].tolist() if 'horse_name' in df.columns else []
        odds_dict = scraper.get_odds(rid, horse_names)

        # äºˆæ¸¬
        X = df[model_features].fillna(-1)
        df['prob'] = model.predict(X)
        df['pred_rank'] = df['prob'].rank(ascending=False, method='min').astype(int)
        df = df.sort_values('prob', ascending=False)

        # ãƒ¬ãƒ¼ã‚¹ç•ªå·æŠ½å‡º
        race_num = rid[-2:]
        race_name = df['race_name'].iloc[0] if 'race_name' in df.columns else f"{race_num}R"
        distance = int(df['distance'].iloc[0]) if 'distance' in df.columns else 0
        start_time = df['start_time'].iloc[0] if 'start_time' in df.columns else ""

        predictions = []
        for i, (_, row) in enumerate(df.iterrows()):  # å…¨é¦¬ã‚’è¿”ã™
            horse_num = int(row['horse_number']) if pd.notna(row.get('horse_number')) else 0
            odds = odds_dict.get(horse_num, 0)
            prob = float(row['prob'])

            # å‹ç‡ãƒ»è¤‡å‹ç‡ã‚’å–å¾—ï¼ˆ0-1ã®ç¯„å›²ã«æ­£è¦åŒ–ï¼‰
            raw_win_rate = float(row.get('horse_win_rate', 0))
            raw_show_rate = float(row.get('horse_show_rate', 0))

            # å€¤ãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆã¯æ—¢ã«ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ãªã®ã§100ã§å‰²ã‚‹
            if raw_win_rate > 1:
                raw_win_rate = raw_win_rate / 100
            if raw_show_rate > 1:
                raw_show_rate = raw_show_rate / 100

            # 0-100%ã®ç¯„å›²ã«ã‚¯ãƒ©ãƒ³ãƒ—
            win_rate = min(max(raw_win_rate * 100, 0), 100)
            show_rate = min(max(raw_show_rate * 100, 0), 100)

            # å¦™å‘³è¨ˆç®—: äºˆæ¸¬ç¢ºç‡ Ã— ã‚ªãƒƒã‚º > 1 ãªã‚‰å¦™å‘³ã‚ã‚Š
            # ä¾‹: äºˆæ¸¬30% Ã— ã‚ªãƒƒã‚º5.0 = 1.5 â†’ æœŸå¾…å€¤ãƒ—ãƒ©ã‚¹
            expected_value = prob * odds if odds > 0 else 0
            is_value = expected_value > 1.0  # æœŸå¾…å€¤1ä»¥ä¸Šãªã‚‰å¦™å‘³ã‚ã‚Š

            predictions.append({
                "rank": i + 1,
                "number": horse_num,
                "name": row.get('horse_name', 'ä¸æ˜'),
                "jockey": row.get('jockey_name', 'ä¸æ˜'),
                "prob": round(prob, 3),
                "win_rate": round(win_rate, 1),
                "show_rate": round(show_rate, 1),
                "odds": odds,
                "expected_value": round(expected_value, 2),
                "is_value": is_value
            })

        results.append({
            "id": race_num,
            "name": race_name,
            "distance": distance,
            "time": start_time,
            "field_size": len(df),  # å‡ºèµ°é ­æ•°ã‚’è¿½åŠ 
            "predictions": predictions
        })

    return {
        "track": {
            "code": track_code,
            "name": TRACKS[track_code]['name'],
            "emoji": TRACKS[track_code]['emoji']
        },
        "date": request.date,
        "races": results
    }


class RaceListRequest(BaseModel):
    track_code: str
    date: str


@app.post("/api/races")
def get_race_list(request: RaceListRequest):
    """ãƒ¬ãƒ¼ã‚¹ä¸€è¦§ã‚’å–å¾—ï¼ˆè»½é‡ï¼‰"""
    track_code = request.track_code
    date_str = request.date.replace("-", "")

    if track_code not in TRACKS:
        raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰")

    scraper = NARScraper(track_code, delay=0.3)
    race_ids = scraper.get_race_list_by_date(date_str)

    return {
        "track": TRACKS[track_code],
        "race_ids": sorted(race_ids)
    }


class SingleRaceRequest(BaseModel):
    race_id: str
    track_code: str


@app.post("/api/predict/race")
def predict_single_race(request: SingleRaceRequest):
    """å˜ä¸€ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬"""
    race_id = request.race_id
    track_code = request.track_code

    if track_code not in TRACKS:
        raise HTTPException(status_code=400, detail="ç„¡åŠ¹ãªç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰")

    model, model_features = load_model(track_code)
    if model is None:
        raise HTTPException(
            status_code=400,
            detail=f"{TRACKS[track_code]['name']}ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“"
        )

    scraper = NARScraper(track_code, delay=0.3)
    processor = Processor()

    df = scraper.get_race_data(race_id)
    if df is None:
        raise HTTPException(status_code=404, detail="ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“")

    df = scraper.enrich_data(df)
    df = processor.process(df)

    # ã‚ªãƒƒã‚ºå–å¾—ï¼ˆé¦¬åãƒªã‚¹ãƒˆã‚’æ¸¡ã—ã¦æ­£ã—ã„ãƒ¬ãƒ¼ã‚¹ã‹ç¢ºèªï¼‰
    horse_names = df['horse_name'].tolist() if 'horse_name' in df.columns else []
    odds_dict = scraper.get_odds(race_id, horse_names)

    # äºˆæ¸¬
    X = df[model_features].fillna(-1)
    df['prob'] = model.predict(X)
    df['pred_rank'] = df['prob'].rank(ascending=False, method='min').astype(int)
    df = df.sort_values('prob', ascending=False)

    # ãƒ¬ãƒ¼ã‚¹æƒ…å ±
    race_num = race_id[-2:]
    race_name = df['race_name'].iloc[0] if 'race_name' in df.columns else f"{race_num}R"
    distance = int(df['distance'].iloc[0]) if 'distance' in df.columns else 0
    start_time = df['start_time'].iloc[0] if 'start_time' in df.columns else ""

    predictions = []
    for i, (_, row) in enumerate(df.iterrows()):  # å…¨é¦¬ã‚’è¿”ã™
        horse_num = int(row['horse_number']) if pd.notna(row.get('horse_number')) else 0
        odds = odds_dict.get(horse_num, 0)
        prob = float(row['prob'])

        # å‹ç‡ãƒ»è¤‡å‹ç‡ã‚’å–å¾—ï¼ˆ0-1ã®ç¯„å›²ã«æ­£è¦åŒ–ï¼‰
        raw_win_rate = float(row.get('horse_win_rate', 0))
        raw_show_rate = float(row.get('horse_show_rate', 0))

        # å€¤ãŒ1ã‚ˆã‚Šå¤§ãã„å ´åˆã¯æ—¢ã«ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ãªã®ã§100ã§å‰²ã‚‹
        if raw_win_rate > 1:
            raw_win_rate = raw_win_rate / 100
        if raw_show_rate > 1:
            raw_show_rate = raw_show_rate / 100

        # 0-100%ã®ç¯„å›²ã«ã‚¯ãƒ©ãƒ³ãƒ—
        win_rate = min(max(raw_win_rate * 100, 0), 100)
        show_rate = min(max(raw_show_rate * 100, 0), 100)

        expected_value = prob * odds if odds > 0 else 0
        is_value = expected_value > 1.0

        predictions.append({
            "rank": i + 1,
            "number": horse_num,
            "name": row.get('horse_name', 'ä¸æ˜'),
            "jockey": row.get('jockey_name', 'ä¸æ˜'),
            "prob": round(prob, 3),
            "win_rate": round(win_rate, 1),
            "show_rate": round(show_rate, 1),
            "odds": odds,
            "expected_value": round(expected_value, 2),
            "is_value": is_value
        })

    return {
        "id": race_num,
        "name": race_name,
        "distance": distance,
        "time": start_time,
        "field_size": len(df),
        "predictions": predictions
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
